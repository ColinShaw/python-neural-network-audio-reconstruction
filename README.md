# Neural Network Audio Reconstruction

These are a variety of ideas about audio signal reconstruction using 
neural networks.  Of course this generalizes to pretty much any time
series, nothing special about audio.  The general idea is that data 
that is known is used as a label, and feature data is generated by 
various means of either reducing the information content of the label
or by adding noise to the label.  The goal, of course, is that the 
networks learns how to fill in the data, remove noise, etc.

The original idea of this is based on my predilection for the Wadia 
digital to analog converters. These use relatively high order polynomial
interpolation to smooth sampled audio data.  Wadia spent a lot of effort 
identifying which algorithms produced results that study participants 
thought sounded better.  The inspiration for this collection of audio
reconstruction experiments is based on the ideas that Wadia was doing 
decades ago, but tempered with the amazing neural network visual 
reconstruction examples that we have today (Alex Champandard's 
[Neural Enhance](https://github.com/alexjc/neural-enhance) being just
one of many examples).

These are just a number of experiments in convenient Jupyter notebooks, 
nothing is a finished product.  You will need `numpy` and `tensorflow`
to run them.  The basic notion of the models is the encoder-decoder, 
but of course this can be broadened to things like the u-net.  The 
generators are quite slow, so don't expect amazing performance, even
on your GPU, since this is not as memory-bound a problem as many 
training exercises.  In order to be performant, the naive implementation 
that I first wrote using `numpy` will need to be changed to using 
native `tensorflow` routines through `keras`.

The first experiment, [NoiseReduction.ipynb](NoiseReduction.ipynb), 
adds two types of noise to a curated signal.  Each sample feature
has randomly generated properties, which include frequency and noise
parameters.  The signal composition is single sinusoid with an integer
number of cycles in the size of the sample.  The amplitude is a random
variable as well.  Gaussian noise is added with random variance per
waveform.  Combined with the fact that the sinusoid amplitude is 
randomly selected, we have random selection of signal-to-noise ratio.  There
is also random spurious noise added.  This noise is a single sample that 
has a fixed amplitude and a 50% chance of being above (or below) the 
sample.  This noise has a probability of occuring, so could be 
considered a Poisson process.  The network is trained using mean squared
error and an Adam optimizer.

The second experiment, [Signal Reconstruction](coming_soon.ipynb), takes
an original signal and quantizes it in time. The reason for this is the 
supposition that the sampling (and time domain quantizing) of the signal 
emulates the process that is present when we record music digitally.  The idea
being that we can train the network to perform the appropriate interpolation.  In
this case, we are using mean-squared-error to drive the training.  The end 
goal is, like in the case of Wadia and their converters, to be able to train
the interpolation based on what cohorts of humans find to be more musical.  This
is not a standard error metric, so there is clearly work to be done!
